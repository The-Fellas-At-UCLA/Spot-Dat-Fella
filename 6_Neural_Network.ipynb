{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcMMxAgO_X8c"
   },
   "source": [
    "\n",
    "# 1. Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bvoIdo1MAsJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Using the URL for the file\n",
    "spotify_original = pd.read_csv(\"spotify_data/dataset.csv\")\n",
    "\n",
    "spotify_original_reshape = spotify_original.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex-ehlXvYZbH"
   },
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "\n",
    "*   Todo 1\n",
    "*   Todo 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1728671738991,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "YqYMe6nK0WLg",
    "outputId": "61c9b59c-88c3-406d-cad7-b75f45b5b948"
   },
   "outputs": [],
   "source": [
    "#spotify_original_reshape.head(20)\n",
    "#spotify_original.shape\n",
    "spotify_original_reshape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1728672778508,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "2k0uIm_Y7R5m",
    "outputId": "2fb8b216-1c1d-48c6-ddce-c4d7e957d668"
   },
   "outputs": [],
   "source": [
    "spotify_original_reshape['track_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1728672723942,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "FaIOFCNvMA69",
    "outputId": "c7125beb-7a89-493e-fb70-1388ea3183e9"
   },
   "outputs": [],
   "source": [
    "spotify_original_reshape['popularity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1728672848289,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "QRHg7rmTMBD_",
    "outputId": "6be119ba-1434-4067-cc04-74cbc14d6795"
   },
   "outputs": [],
   "source": [
    "spotify_original_reshape['track_genre'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1728675288832,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "UGZ8pMoLExvK",
    "outputId": "2189e617-9f20-4833-83c4-74f23e43a8f7"
   },
   "outputs": [],
   "source": [
    "missing_track = spotify_original_reshape[spotify_original_reshape['track_id'] == '1kR4gIb7nGxHPI3D2ifs59']\n",
    "print(missing_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1728755806807,
     "user": {
      "displayName": "SHAFEE KHAN",
      "userId": "10219242836598942056"
     },
     "user_tz": 420
    },
    "id": "yBslr9RFK4D4",
    "outputId": "b290b10a-6464-41b1-ab51-fe28ebd93161"
   },
   "outputs": [],
   "source": [
    "# Cleaning rows with missing information\n",
    "missing_data_rows = spotify_original_reshape[spotify_original_reshape.isnull().any(axis=1)]\n",
    "\n",
    "missing_data_rows\n",
    "\n",
    "spotify_original_reshape_drop = spotify_original_reshape.dropna()\n",
    "\n",
    "print(spotify_original_reshape.shape)\n",
    "print(spotify_original_reshape_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1728755928926,
     "user": {
      "displayName": "SHAFEE KHAN",
      "userId": "10219242836598942056"
     },
     "user_tz": 420
    },
    "id": "2xASmZeljbVN",
    "outputId": "3340b028-92e1-4ef3-b247-63957fa81263"
   },
   "outputs": [],
   "source": [
    "#clean track_name and artists columns by stripping spaces and converting to lowercase\n",
    "spotify_original_reshape_drop['track_name_clean'] = spotify_original_reshape_drop['track_name'].str.strip().str.lower()\n",
    "spotify_original_reshape_drop['artists_clean'] = spotify_original_reshape_drop['artists'].str.strip().str.lower()\n",
    "\n",
    "#priority list for genres to handle duplicates\n",
    "genre_priority = ['pop', 'rock', 'hip hop', 'rap', 'reggaeton', 'latin', 'electronic', 'r&b', 'reggae', 'dance', 'classical']\n",
    "spotify_original_reshape_drop['genre_priority'] = spotify_original_reshape_drop['track_genre'].apply(lambda x: genre_priority.index(x) if x in genre_priority else len(genre_priority))\n",
    "\n",
    "#sort the dataset by track_name, artists, genre priority, popularity, and duration\n",
    "spotify_data_sorted = spotify_original_reshape_drop.sort_values(by=['track_name_clean', 'artists_clean', 'genre_priority', 'popularity', 'duration_ms'],\n",
    "                                                                ascending=[True, True, True, True, False])\n",
    "\n",
    "#remove duplicates\n",
    "spotify_cleaned = spotify_data_sorted.drop_duplicates(subset=['track_name_clean', 'artists_clean'], keep='first')\n",
    "\n",
    "# checking size\n",
    "print(f\"Shape of the dataset before cleaning: {spotify_original_reshape_drop.shape}\")\n",
    "print(f\"Shape of the dataset after cleaning: {spotify_cleaned.shape}\")\n",
    "\n",
    "# removing extra columns added\n",
    "spotify_cleaned_final = spotify_cleaned.drop(columns=['track_name_clean', 'artists_clean', 'genre_priority'])\n",
    "\n",
    "# Fcheck size again\n",
    "print(f\"Shape of the dataset after removing extra columns: {spotify_cleaned_final.shape}\")\n",
    "\n",
    "\n",
    "spotify_cleaned_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WK1C3rzANxG"
   },
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1728672579553,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "aca4HVPZ543-",
    "outputId": "3fd4abdc-6ebb-4947-e72a-1ec5b6d4810b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Histograms\n",
    "\"\"\"\n",
    "spotify_original_reshape['liveness'].hist(bins = 30, alpha = 0.5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1728675072745,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "SLHOJ7WGB0Gb",
    "outputId": "536e1583-d161-46a2-944a-f6e979574ce4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing correlation between dancebility and popularity features\n",
    "\"\"\"\n",
    "\n",
    "plt.scatter(spotify_original_reshape['danceability'], spotify_original_reshape['popularity'], s = 0.1)\n",
    "plt.xlabel('Danceability')\n",
    "plt.ylabel('Popularity')\n",
    "plt.title('Danceability vs Popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1728675405574,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "swljig2WFNDb",
    "outputId": "fbd5a8c4-6b30-4529-9b6b-328c3fd800da"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing correlation between energy and popularity\n",
    "\"\"\"\n",
    "\n",
    "plt.scatter(spotify_original_reshape['energy'], spotify_original_reshape['popularity'], s = 0.1)\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('Popularity')\n",
    "plt.title('Energy vs Popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1728676132293,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "5eKIrGgEF2Zh",
    "outputId": "dc9dbabb-e937-488e-d941-9a7358cab4fe"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing the Correlation Between Popularity and other features\n",
    "\"\"\"\n",
    "\n",
    "for features in spotify_original_reshape.select_dtypes(include=[np.number]).columns:\n",
    "  print(features, 'vs. Popularity Correlation:', np.corrcoef(spotify_original_reshape['popularity'], spotify_original_reshape[features])[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1728676225815,
     "user": {
      "displayName": "PAUL CANO",
      "userId": "16231630220263022191"
     },
     "user_tz": 420
    },
    "id": "XT_JFtOLIT3h",
    "outputId": "385e3a7d-17c1-4750-f50c-93cb12fead91"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing the Correlation Between Tempo and other features\n",
    "\"\"\"\n",
    "\n",
    "for features in spotify_original_reshape.select_dtypes(include=[np.number]).columns:\n",
    "  print(features, 'vs. Tempo Correlation:', np.corrcoef(spotify_original_reshape['tempo'], spotify_original_reshape[features])[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Correlation matrix and Heatmap\n",
    "\"\"\"\n",
    "\n",
    "numeric_data = spotify_cleaned_final.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "danceability_correlation = correlation_matrix[\"danceability\"].sort_values(ascending=False)\n",
    "\n",
    "print(danceability_correlation)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a heatmap to visualize correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap for Spotify Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spotify_cleaned_final.shape\n",
    "\n",
    "numeric_columns = spotify_cleaned_final.select_dtypes(include=[np.number]).columns\n",
    "non_numeric_columns = spotify_cleaned_final.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "print(f\"Numeric columns ({len(numeric_columns)}): {numeric_columns.tolist()}\")\n",
    "print(f\"Non-numeric columns ({len(non_numeric_columns)}): {non_numeric_columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_data = spotify_cleaned_final.select_dtypes(include=[np.number])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(numeric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None) \n",
    "pca.fit(data_standardized)\n",
    "\n",
    "data_pca = pca.transform(data_standardized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the standardized dataset\n",
    "pca_U, pca_d, pca_V = np.linalg.svd(data_standardized, full_matrices=False)\n",
    "\n",
    "explained_variance = (pca_d ** 2) / (len(data_standardized) - 1)\n",
    "\n",
    "total_variance = np.sum(explained_variance)\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(pca_d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_var = np.square(pca_d) / sum(np.square(pca_d))\n",
    "pd.DataFrame(\n",
    "    {\"PC\": 1 + np.arange(0, prop_var.shape[0]),\n",
    "     \"variability_explained\": prop_var.round(2),\n",
    "     \"cumulative_variability_explained\": prop_var.cumsum().round(2)\n",
    "     }).head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings1 = pd.DataFrame(\n",
    "    {\n",
    "        \"features\": spotify_cleaned_final_numeric.columns, \n",
    "        \"pc1_loading\": pca_V[0] \n",
    "    }\n",
    ")\n",
    "\n",
    "loadings1.reindex(loadings1[\"pc1_loading\"].abs().sort_values(ascending=False).index) \\\n",
    "    .head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings2 = pd.DataFrame(\n",
    "    {\"features\": spotify_cleaned_final_numeric.columns,\n",
    "     \"pc2_loading\": pca_V[1]\n",
    "     })\n",
    "# look at the 10 largest (absolute value) loadings for PC2 but print out the signed value\n",
    "loadings2.reindex(loadings2[\"pc2_loading\"].abs().sort_values(ascending=False).index) \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pca_scaled_x = data_standardized @ pca_V.T\n",
    "\n",
    "pca_scaled_x = pd.DataFrame(pca_scaled_x)\n",
    "\n",
    "pca_scaled_x.columns = [\"PC\" + str(1 + col) for col in range(pca_scaled_x.shape[1])]\n",
    "\n",
    "pca_scaled_x.index = spotify_cleaned_final['track_name'] \n",
    "\n",
    "pca_scaled_x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_scaled_x['PC1'], pca_scaled_x['PC2'], alpha=0.7)\n",
    "\n",
    "\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.title('PCA: PC1 vs PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(pca_scaled_x['PC1'], pca_scaled_x['PC2'], c=spotify_cleaned_final_numeric['energy'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Energy')\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.title('PCA: PC1 vs PC2 (Colored by Energy)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_prop_explained = np.square(pca_d) / sum(np.square(pca_d))\n",
    "\n",
    "# Create a line plot showing the proportion of variance explained by each principal component\n",
    "fig = px.line(\n",
    "    x=np.arange(1, pca_prop_explained.shape[0] + 1),  # PC indices start at 1\n",
    "    y=pca_prop_explained,\n",
    "    labels={\"x\": \"Principal Component (PC)\", \"y\": \"Proportion of Variance Explained\"},\n",
    "    title=\"Proportion of Variance Explained by Each Principal Component\"\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_prop_explained)\n",
    "\n",
    "# Create a line plot showing the cumulative variance explained by each component\n",
    "fig = px.line(\n",
    "    x=np.arange(1, cumulative_variance.shape[0] + 1),\n",
    "    y=cumulative_variance,\n",
    "    labels={\"x\": \"Principal Component (PC)\", \"y\": \"Cumulative Variance Explained\"},\n",
    "    title=\"Cumulative Variance Explained by Principal Components\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = data_standardized @ pca_V[:7].T\n",
    "reduced_data_df = pd.DataFrame(reduced_data, columns=[f'PC{i+1}' for i in range(7)])\n",
    "\n",
    "reduced_data_df.index = spotify_cleaned_final['track_name']\n",
    "reduced_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Scatter plot of PC1 vs PC2\n",
    "fig = px.scatter(\n",
    "    reduced_data_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hover_name=reduced_data_df.index,\n",
    "    title=\"PCA Scatter Plot: PC1 vs PC2\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanations: \n",
    "\n",
    "1. For this project check-in your team must demonstrate at least one unsupervised learning method: PCA or clustering. You may even combine them.\n",
    "\n",
    "    *We chose to apply PCA to our dataset.*\n",
    "\n",
    "2. If you apply PCA to your data, include code, cumulative variability explained, and scree plot. Explain how you are using PCA in your project for dimensionality reduction or to learn structure in the data.\n",
    "\n",
    "    *We used PCA to reduce the dataset from 14 features to 7 principal components. We projected the data to the first 7 PC, and we determined which PC were the most influential using the scree plot and the cumulative variance plot*\n",
    "\n",
    "3. If you apply clustering, include code. quantitative metrics to evaluate clustering, and how you determined the number of clusters for your data and why. Explain how clustering helped you learn about the structure of data for your project.\n",
    "\n",
    "    *N/A*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "* We want to predict the mode of a song using neural network with 4 hidden layers and 1 output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "* Here we will drop track_id, track_name, artists, popularity because they are irrelevant to predicting the mode of a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# drop track_id, track_name, artists, album_name, track_genre, mode, and popularity columns\n",
    "x_train = spotify_cleaned_final.drop(columns=['track_id', 'track_name', 'artists', 'track_genre', 'popularity', 'mode', 'album_name'])\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the standardized dataset\n",
    "pca_U, pca_d, pca_V = np.linalg.svd(x_train, full_matrices=False)\n",
    "\n",
    "prop_var = np.square(pca_d) / sum(np.square(pca_d))\n",
    "print(prop_var)\n",
    "\n",
    "# PC transformation\n",
    "x_train_pca = x_train @ pca_V.T\n",
    "\n",
    "# pick the first 10 principal components\n",
    "x_train_pca = x_train[:, :10]\n",
    "\n",
    "# create tensor from the standardized data\n",
    "x_train_pca = torch.tensor(x_train_pca, dtype=torch.float32)\n",
    "y_train_pca = torch.tensor(spotify_cleaned_final['mode'].values, dtype=torch.float32)\n",
    "input_pca_size = x_train_pca.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network with 4 hidden layers and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MultiLayerNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  # Input to first hidden layer\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)  # First to second hidden layer\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)  # Second to third hidden layer\n",
    "        self.layer4 = nn.Linear(hidden_size, hidden_size)  # Third to fourth hidden layer\n",
    "        self.layer5 = nn.Linear(hidden_size, hidden_size)  # Fourth to fifth hidden layer\n",
    "        self.layer6 = nn.Linear(hidden_size, hidden_size)  # Fifth to sixth hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)  # Fourth to output layer\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        x = self.sigmoid(self.layer4(x))    \n",
    "        x = self.sigmoid(self.layer5(x))\n",
    "        x = self.sigmoid(self.layer6(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with PC inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss_Fn, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerNN(input_size=input_pca_size, hidden_size=16, output_size=1)\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\"\\n#################### Training the neural network ####################\\n\")\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  prediction = model(x_train_pca)\n",
    "  loss = loss_function(prediction, y_train_pca.view(-1, 1))\n",
    "\n",
    "  # Backward pass\n",
    "  optimizer.zero_grad() # Zero out the gradients from the previous iteration\n",
    "  loss.backward() # Compute the gradients for each parameter\n",
    "  optimizer.step() # Update the parameters using the gradients\n",
    "\n",
    "  # Print the loss every 10 epochs\n",
    "  if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Finder\n",
    "lrs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "losses = []\n",
    "\n",
    "for lr in lrs:\n",
    "    model = MultiLayerNN(input_size=input_pca_size, hidden_size=16, output_size=1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        prediction = model(x_train_pca)\n",
    "        loss = loss_function(prediction, y_train_pca.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(lrs, losses)\n",
    "# label the axes\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance with LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with lr=0.1\n",
    "model = MultiLayerNN(input_size=input_pca_size, hidden_size=16, output_size=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    prediction = model(x_train_pca)\n",
    "    loss = loss_function(prediction, y_train_pca.view(-1, 1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model with accuracy, precision, recall, and F1 score\n",
    "prediction = model(x_train)\n",
    "prediction = torch.round(prediction)\n",
    "y_train = y_train.view(-1, 1)\n",
    "\n",
    "true_positives = torch.sum(prediction[y_train == 1] == 1).item()\n",
    "false_positives = torch.sum(prediction[y_train == 0] == 1).item()\n",
    "true_negatives = torch.sum(prediction[y_train == 0] == 0).item()\n",
    "false_negatives = torch.sum(prediction[y_train == 1] == 0).item()\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / len(y_train)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "confusion_matrix[0, 0] = true_negatives\n",
    "confusion_matrix[0, 1] = false_positives\n",
    "confusion_matrix[1, 0] = false_negatives\n",
    "confusion_matrix[1, 1] = true_positives\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4WK1C3rzANxG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
