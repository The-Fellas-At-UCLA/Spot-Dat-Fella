{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcMMxAgO_X8c"
   },
   "source": [
    "\n",
    "# 1. Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bvoIdo1MAsJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Using the URL for the file\n",
    "spotify_original = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "spotify_original_reshape = spotify_original.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex-ehlXvYZbH"
   },
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning rows with missing information\n",
    "missing_data_rows = spotify_original_reshape[spotify_original_reshape.isnull().any(axis=1)]\n",
    "\n",
    "missing_data_rows\n",
    "\n",
    "spotify_original_reshape_drop = spotify_original_reshape.dropna()\n",
    "\n",
    "print(spotify_original_reshape.shape)\n",
    "print(spotify_original_reshape_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean track_name and artists columns by stripping spaces and converting to lowercase\n",
    "spotify_original_reshape_drop['track_name_clean'] = spotify_original_reshape_drop['track_name'].str.strip().str.lower()\n",
    "spotify_original_reshape_drop['artists_clean'] = spotify_original_reshape_drop['artists'].str.strip().str.lower()\n",
    "\n",
    "#priority list for genres to handle duplicates\n",
    "genre_priority = ['pop', 'rock', 'hip hop', 'rap', 'reggaeton', 'latin', 'electronic', 'r&b', 'reggae', 'dance', 'classical']\n",
    "spotify_original_reshape_drop['genre_priority'] = spotify_original_reshape_drop['track_genre'].apply(lambda x: genre_priority.index(x) if x in genre_priority else len(genre_priority))\n",
    "\n",
    "#sort the dataset by track_name, artists, genre priority, popularity, and duration\n",
    "spotify_data_sorted = spotify_original_reshape_drop.sort_values(by=['track_name_clean', 'artists_clean', 'genre_priority', 'popularity', 'duration_ms'],\n",
    "                                                                ascending=[True, True, True, True, False])\n",
    "\n",
    "#remove duplicates\n",
    "spotify_cleaned = spotify_data_sorted.drop_duplicates(subset=['track_name_clean', 'artists_clean'], keep='first')\n",
    "\n",
    "# checking size\n",
    "print(f\"Shape of the dataset before cleaning: {spotify_original_reshape_drop.shape}\")\n",
    "print(f\"Shape of the dataset after cleaning: {spotify_cleaned.shape}\")\n",
    "\n",
    "# removing extra columns added\n",
    "spotify_cleaned_final = spotify_cleaned.drop(columns=['track_name_clean', 'artists_clean', 'genre_priority'])\n",
    "\n",
    "# Fcheck size again\n",
    "print(f\"Shape of the dataset after removing extra columns: {spotify_cleaned_final.shape}\")\n",
    "\n",
    "\n",
    "spotify_cleaned_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge Genres\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "genre_mapping = {\n",
    "    # Pop\n",
    "    'pop': 'Pop', 'power-pop': 'Pop', 'synth-pop': 'Pop', 'indie-pop': 'Pop',\n",
    "    'k-pop': 'Pop', 'j-pop': 'Pop', 'cantopop': 'Pop', 'mandopop': 'Pop',\n",
    "    'british': 'Pop', 'spanish': 'Pop', 'latino': 'Pop', 'pop-film': 'Pop',\n",
    "    'pagode': 'Pop', 'j-idol': 'Pop', 'sad': 'Pop',\n",
    "    \n",
    "    # Rock\n",
    "    'rock': 'Rock', 'rock-n-roll': 'Rock', 'alt-rock': 'Rock', 'indie': 'Rock',\n",
    "    'hard-rock': 'Rock', 'punk-rock': 'Rock', 'garage': 'Rock', 'psych-rock': 'Rock',\n",
    "    'grunge': 'Rock', 'guitar': 'Rock', 'ska': 'Rock', 'emo': 'Rock', 'punk': 'Rock',\n",
    "    'death-metal': 'Rock', 'hardcore': 'Rock', 'metal': 'Rock', 'heavy-metal': 'Rock',\n",
    "    'black-metal': 'Rock', 'metalcore': 'Rock', 'j-rock': 'Electronic/Dance', 'rockabilly': 'Rock',\n",
    "    'alternative': 'Rock',\n",
    "    \n",
    "    # Electronic/Dance\n",
    "    'electronic': 'Electronic/Dance', 'edm': 'Electronic/Dance', 'house': 'Electronic/Dance',\n",
    "    'deep-house': 'Electronic/Dance', 'progressive-house': 'Electronic/Dance',\n",
    "    'techno': 'Electronic/Dance', 'minimal-techno': 'Electronic/Dance',\n",
    "    'detroit-techno': 'Electronic/Dance', 'trance': 'Electronic/Dance',\n",
    "    'dubstep': 'Electronic/Dance', 'drum-and-bass': 'Electronic/Dance',\n",
    "    'breakbeat': 'Electronic/Dance', 'club': 'Electronic/Dance',\n",
    "    'dancehall': 'Electronic/Dance', 'j-dance': 'Electronic/Dance', 'disco': 'Electronic/Dance',\n",
    "    'hardstyle': 'Electronic/Dance', 'chill': 'Electronic/Dance',\n",
    "    'electro': 'Electronic/Dance', 'dance': 'Electronic/Dance',\n",
    "    \n",
    "    # Hip-Hop/R&B\n",
    "    'hip-hop': 'Hip-Hop/R&B', 'r-n-b': 'Hip-Hop/R&B', 'funk': 'Hip-Hop/R&B',\n",
    "    'afrobeat': 'Hip-Hop/R&B', 'reggaeton': 'Hip-Hop/R&B', 'dub': 'Hip-Hop/R&B',\n",
    "    'groove': 'Hip-Hop/R&B', 'reggae': 'Hip-Hop/R&B',\n",
    "    \n",
    "    # Jazz/Blues\n",
    "    'jazz': 'Jazz/Blues', 'blues': 'Jazz/Blues', 'bluegrass': 'Jazz/Blues',\n",
    "    'gospel': 'Jazz/Blues', 'soul': 'Jazz/Blues',\n",
    "    \n",
    "    # Classical/Instrumental\n",
    "    'classical': 'Classical/Instrumental', 'piano': 'Classical/Instrumental',\n",
    "    'opera': 'Classical/Instrumental', 'ambient': 'Classical/Instrumental',\n",
    "    'idm': 'Classical/Instrumental', 'trip-hop': 'Classical/Instrumental',\n",
    "    'new-age': 'Classical/Instrumental', 'singer-songwriter': 'Classical/Instrumental',\n",
    "    'study': 'Classical/Instrumental',\n",
    "    \n",
    "    # Folk/World\n",
    "    'folk': 'Folk/World', 'acoustic': 'Folk/World', 'country': 'Folk/World',\n",
    "    'honky-tonk': 'Folk/World', 'turkish': 'Folk/World', 'brazil': 'Folk/World',\n",
    "    'samba': 'Folk/World', 'forro': 'Folk/World', 'indian': 'Folk/World',\n",
    "    'iranian': 'Folk/World', 'malay': 'Folk/World', 'afrobeat': 'Folk/World',\n",
    "    'world-music': 'Folk/World',\n",
    "    \n",
    "    # Latin\n",
    "    'latin': 'Latin', 'salsa': 'Latin', 'tango': 'Latin', 'pagode': 'Latin',\n",
    "    'mpb': 'Latin', 'sertanejo': 'Latin', 'brazil': 'Latin',\n",
    "    \n",
    "    # Children's/Family\n",
    "    'kids': 'Children\\'s/Family', 'children': 'Children\\'s/Family', 'disney': 'Children\\'s/Family',\n",
    "    'show-tunes': 'Children\\'s/Family', 'romance': 'Children\\'s/Family', 'happy': 'Children\\'s/Family',\n",
    "    \n",
    "    # Experimental/Alternative\n",
    "    'industrial': 'Experimental/Alternative', 'grindcore': 'Experimental/Alternative',\n",
    "    'goth': 'Experimental/Alternative', 'detroit-techno': 'Experimental/Alternative',\n",
    "    'idm': 'Experimental/Alternative',\n",
    "    \n",
    "    # Other/Functional\n",
    "    'anime': 'Other/Functional', 'study': 'Other/Functional', 'party': 'Other/Functional',\n",
    "    'sleep': 'Other/Functional', 'comedy': 'Other/Functional', 'french': 'Other/Functional',\n",
    "    'german': 'Other/Functional', 'swedish': 'Other/Functional', 'chicago-house': 'Other/Functional'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "spotify_cleaned_final['merged_genre'] = spotify_cleaned_final['track_genre'].map(genre_mapping)\n",
    "\n",
    "# drop track_genre column\n",
    "spotify_cleaned_final = spotify_cleaned_final.drop(columns=['track_genre'])\n",
    "\n",
    "# Add one-hot encoding for merged_genre\n",
    "spotify_cleaned_final = pd.get_dummies(spotify_cleaned_final, columns=['merged_genre'])\n",
    "print(spotify_cleaned_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model to predict the popularity of a song based on its audio features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "spotify_processed = spotify_cleaned_final\n",
    "\n",
    "# Process popularity > 30 as popular, otherwise unpopular\n",
    "spotify_processed['popularity'] = spotify_processed['popularity'].apply(lambda x: 1 if x > 35 else 0)\n",
    "print(spotify_processed['popularity'].value_counts())\n",
    "\n",
    "Y = spotify_processed['popularity']\n",
    "# X = spotify_processed[['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "#                        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "#                        'time_signature', 'merged_genre_Classical/Instrumental', \n",
    "#                        'merged_genre_Electronic/Dance', 'merged_genre_Folk/World', 'merged_genre_Hip-Hop/R&B', \n",
    "#                        'merged_genre_Jazz/Blues', 'merged_genre_Latin', 'merged_genre_Other/Functional', \n",
    "#                        'merged_genre_Pop', 'merged_genre_Rock']]\n",
    "X = spotify_processed[['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "                       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                       'time_signature']]\n",
    "\n",
    "numerical_features = ['danceability', 'energy', 'key', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
    "category_features = ['mode', 'merged_genre_Classical/Instrumental', 'merged_genre_Electronic/Dance', 'merged_genre_Folk/World', 'merged_genre_Hip-Hop/R&B', 'merged_genre_Jazz/Blues', 'merged_genre_Latin', 'merged_genre_Other/Functional', 'merged_genre_Pop', 'merged_genre_Rock']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Don't Standardize because it does not affect the performance of Decision Trees\n",
    "# scaler = StandardScaler()\n",
    "# X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "# X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the model, compare train vs validation performance\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the model on the Train set\n",
    "print(\"Train set:\") \n",
    "train_predictions = model.predict(X_train)\n",
    "train_accuracy = metrics.accuracy_score(Y_train, train_predictions)\n",
    "print(f\"Accuracy: {train_accuracy}\")\n",
    "confusion_matrix = metrics.confusion_matrix(Y_train, train_predictions)\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
    "precision = metrics.precision_score(Y_train, train_predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = metrics.recall_score(Y_train, train_predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = metrics.f1_score(Y_train, train_predictions)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Evaluate the model on the Test set\n",
    "print(\"\\nTest set:\")\n",
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = metrics.accuracy_score(Y_test, test_predictions)\n",
    "print(f\"Accuracy: {test_accuracy}\")\n",
    "confusion_matrix = metrics.confusion_matrix(Y_test, test_predictions)\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
    "precision = metrics.precision_score(Y_test, test_predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = metrics.recall_score(Y_test, test_predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = metrics.f1_score(Y_test, test_predictions)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forest\n",
    "- Consider that the training set and validation set performance is too different, we need to use random forest to mitigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Random Forest Model\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=8743)\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
    "print(f\"Average Tree Depth: {np.mean(tree_depths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the Random Forest Model on the Train set\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the model on the Train set\n",
    "print(\"##### Train set: #####\")\n",
    "train_predictions = rf_model.predict(X_train)\n",
    "train_accuracy = metrics.accuracy_score(Y_train, train_predictions)\n",
    "print(f\"Accuracy: {train_accuracy}\")\n",
    "train_conf_matrix = metrics.confusion_matrix(Y_train, train_predictions)\n",
    "print(f\"Confusion matrix:\\n{train_conf_matrix}\")\n",
    "precision = metrics.precision_score(Y_train, train_predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = metrics.recall_score(Y_train, train_predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = metrics.f1_score(Y_train, train_predictions)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Cross-Validation to compare RF performance against training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross-Validation\n",
    "\"\"\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Convert results to numpy arrays for easy calculation of averages\n",
    "accuracies = np.array(accuracies)\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "print(\"\\n##### Cross-validation results: #####\")\n",
    "print(f\"Accuracy: Mean={accuracies.mean():.4f}, Std={accuracies.std():.4f}\")\n",
    "print(f\"Precision: Mean={precisions.mean():.4f}, Std={precisions.std():.4f}\")\n",
    "print(f\"Recall: Mean={recalls.mean():.4f}, Std={recalls.std():.4f}\")\n",
    "print(f\"F1 Score: Mean={f1_scores.mean():.4f}, Std={f1_scores.std():.4f}\")\n",
    "\n",
    "# Example: Print confusion matrices for each fold\n",
    "for i, conf_matrix in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{conf_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper-Parameters Tuning\n",
    "\"\"\"\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],            # Maximum depth of each tree\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Perform Grid Search with 5-fold Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    scoring='accuracy',         # Use accuracy as the evaluation metric\n",
    "    n_jobs=-1,                  # Use all available cores\n",
    "    verbose=2                   # Show progress logs\n",
    ")\n",
    "\n",
    "# Fit the model on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from Grid Search\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Best cross-validation score\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new parameters, we create a new model and find the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = RandomForestClassifier(n_estimators=200, max_depth=20, max_features='sqrt', random_state=8743)\n",
    "new_model.fit(X_train, Y_train)\n",
    "\n",
    "impurity_importances = pd.DataFrame({ 'variable': X_train.columns, 'importance': new_model.feature_importances_ })\n",
    "print(\"\\nVariable Importances:\")\n",
    "impurity_importances = impurity_importances.sort_values('importance', ascending=False)\n",
    "print(impurity_importances)\n",
    "\n",
    "# Plot the variable importances as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(impurity_importances['variable'], impurity_importances['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Variable Importances')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the Train set\n",
    "print(\"##### Train set: #####\")\n",
    "train_predictions = new_model.predict(X_train)\n",
    "train_accuracy = metrics.accuracy_score(Y_train, train_predictions)\n",
    "print(f\"Accuracy: {train_accuracy}\")\n",
    "train_conf_matrix = metrics.confusion_matrix(Y_train, train_predictions)\n",
    "print(f\"Confusion matrix:\\n{train_conf_matrix}\")\n",
    "precision = metrics.precision_score(Y_train, train_predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = metrics.recall_score(Y_train, train_predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = metrics.f1_score(Y_train, train_predictions)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "# Evaluate the model on the Test set\n",
    "print(\"\\n##### Test set: #####\")\n",
    "test_predictions = new_model.predict(X_test)\n",
    "test_accuracy = metrics.accuracy_score(Y_test, test_predictions)\n",
    "print(f\"Accuracy: {test_accuracy}\")\n",
    "test_conf_matrix = metrics.confusion_matrix(Y_test, test_predictions)\n",
    "print(f\"Confusion matrix:\\n{test_conf_matrix}\")\n",
    "precision = metrics.precision_score(Y_test, test_predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = metrics.recall_score(Y_test, test_predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = metrics.f1_score(Y_test, test_predictions)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4WK1C3rzANxG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
